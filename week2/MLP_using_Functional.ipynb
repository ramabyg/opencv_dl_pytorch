{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2cvA6soQQFV"
   },
   "source": [
    "# MLP with Single Hidden Layer using PyTorch\n",
    "\n",
    "1. Define an MLP with variable number of inputs (num_inputs), outputs (num_outputs), and nodes in hidden layer (num_hidden_layer_nodes).  \n",
    "2. Use ReLU activation for each node \n",
    "3. Use MSE loss\n",
    "4. Use SGD optimizer\n",
    "\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/mlp.png\" alt=\"mlp\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PPyTXeGJQcuY",
    "outputId": "81d37dac-ed1a-4cd6-c51f-0be62d487340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10581.46484375\n",
      "1 9755.71875\n",
      "2 9161.30078125\n",
      "3 8637.4951171875\n",
      "4 8135.08154296875\n",
      "5 7634.916015625\n",
      "6 7127.32080078125\n",
      "7 6610.970703125\n",
      "8 6087.37255859375\n",
      "9 5563.396484375\n",
      "10 5048.94873046875\n",
      "11 4551.427734375\n",
      "12 4078.5126953125\n",
      "13 3634.7451171875\n",
      "14 3225.676025390625\n",
      "15 2852.95361328125\n",
      "16 2516.31201171875\n",
      "17 2214.47265625\n",
      "18 1946.53515625\n",
      "19 1709.1044921875\n",
      "20 1499.0308837890625\n",
      "21 1313.60205078125\n",
      "22 1151.3802490234375\n",
      "23 1009.0332641601562\n",
      "24 883.9778442382812\n",
      "25 774.8358764648438\n",
      "26 679.2198486328125\n",
      "27 596.10205078125\n",
      "28 524.0062866210938\n",
      "29 462.0333557128906\n",
      "30 410.55908203125\n",
      "31 370.9834899902344\n",
      "32 349.6309814453125\n",
      "33 361.6949462890625\n",
      "34 444.9823913574219\n",
      "35 685.0133666992188\n",
      "36 1272.060302734375\n",
      "37 2519.0810546875\n",
      "38 4693.56982421875\n",
      "39 6812.61962890625\n",
      "40 6350.89111328125\n",
      "41 3024.792724609375\n",
      "42 1011.534912109375\n",
      "43 448.4607238769531\n",
      "44 291.2796325683594\n",
      "45 218.34922790527344\n",
      "46 172.93954467773438\n",
      "47 140.8631134033203\n",
      "48 116.91952514648438\n",
      "49 98.41030883789062\n",
      "50 83.75048828125\n",
      "51 71.90704345703125\n",
      "52 62.20292282104492\n",
      "53 54.14150619506836\n",
      "54 47.40020751953125\n",
      "55 41.67863464355469\n",
      "56 36.79069900512695\n",
      "57 32.58918380737305\n",
      "58 28.953670501708984\n",
      "59 25.813020706176758\n",
      "60 23.07463836669922\n",
      "61 20.670194625854492\n",
      "62 18.56175994873047\n",
      "63 16.700477600097656\n",
      "64 15.052255630493164\n",
      "65 13.593114852905273\n",
      "66 12.298100471496582\n",
      "67 11.141427040100098\n",
      "68 10.104217529296875\n",
      "69 9.184833526611328\n",
      "70 8.358495712280273\n",
      "71 7.615204334259033\n",
      "72 6.948360919952393\n",
      "73 6.3468241691589355\n",
      "74 5.806619644165039\n",
      "75 5.318169116973877\n",
      "76 4.876824378967285\n",
      "77 4.475938320159912\n",
      "78 4.113371849060059\n",
      "79 3.7841951847076416\n",
      "80 3.4850587844848633\n",
      "81 3.211879253387451\n",
      "82 2.9633195400238037\n",
      "83 2.7361021041870117\n",
      "84 2.52987003326416\n",
      "85 2.3401002883911133\n",
      "86 2.166550397872925\n",
      "87 2.0085182189941406\n",
      "88 1.862983226776123\n",
      "89 1.7293310165405273\n",
      "90 1.6063308715820312\n",
      "91 1.4932509660720825\n",
      "92 1.389578104019165\n",
      "93 1.2937796115875244\n",
      "94 1.2057088613510132\n",
      "95 1.124582052230835\n",
      "96 1.0492390394210815\n",
      "97 0.9798553586006165\n",
      "98 0.9156844615936279\n",
      "99 0.8562864065170288\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden_layer_nodes, num_outputs):\n",
    "        # Initialize super class\n",
    "        super().__init__()\n",
    "\n",
    "        # Add hidden layer\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden_layer_nodes)\n",
    "\n",
    "        # Add output layer\n",
    "        self.linear2 = nn.Linear(num_hidden_layer_nodes, num_outputs)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through hidden layer with\n",
    "        x = F.relu(self.linear1(x))\n",
    "\n",
    "        # Foward pass to output layer\n",
    "        return self.linear2(x)\n",
    "\n",
    "# Num data points\n",
    "num_data = 1000\n",
    "\n",
    "# Network parameters\n",
    "num_inputs = 1000\n",
    "num_hidden_layer_nodes = 100\n",
    "num_outputs = 10\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(num_data, num_inputs)\n",
    "y = torch.randn(num_data, num_outputs)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = MLP(num_inputs, num_hidden_layer_nodes, num_outputs)\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for t in range(num_epochs):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate gradient using backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters (weights)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyjPulcDSJQo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MLP-without-sequential",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
